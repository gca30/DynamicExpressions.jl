        - module TensorNodeUtilsModule
        - 
        - import Compat: Returns
        - import ..NodeModule:
        -     AbstractNode,
        -     AbstractExprNode,
        -     AbstractTensorExprNode,
        -     Node,
        -     preserve_sharing,
        -     constructorof,
        -     copy_node,
        -     count_nodes,
        -     tree_mapreduce,
        -     any,
        -     filter_map
        - import ..FlattenedTensorListModule:
        -     FlattenedTensorList,
        -     permute_features!,
        -     treat_as_flattened,
        -     feature_flat,
        -     copy_ti!,
        -     mapk_ti!,
        -     selectdim_ti
        - using ..NodeUtilsModule:
        -     count_constant_nodes,
        -     is_node_constant
        - 
        - function recalculate_constant_indices!(tree::AbstractTensorExprNode{T,N}, constants::FlattenedTensorList{T,N}) where {T,N}
       64     function recurse(node, cindex, v)
        0         if node.degree == 2
        0             cindex = recurse(node.l, cindex, v)
        0             cindex = recurse(node.r, cindex, v)
        0         elseif node.degree == 1
        0             cindex = recurse(node.l, cindex, v)
        0         elseif node.degree == 0 && node.constant
        0             v[cindex] = node.feature
        0             node.feature = cindex
        0             return cindex + 1
        -         end
        0         return cindex
        -     end
        0     count_consts = count_constant_nodes(tree)
      128     v = Vector{Int32}(undef, count_consts)
        0     recurse(tree, 1, v)
        0     for i in eachindex(v)
        0         if i != v[i]
        0             permute_features!(constants, v)
        -             return
        -         end
        0     end
        - end
        - 
        - # renumbers the nodes to be from 1 to the number of temporary nodes (meaning inputs and constants are not numbered)
        - # this removes the information of the original constant indices
        - function recalculate_node_indices!(tree::AbstractTensorExprNode)
      128     function recurse(node, nfeature, nindex) 
        0         if node.degree == 2
      512             nfeature, nindex = recurse(node.l, nfeature, nindex)
      512             nfeature, nindex = recurse(node.r, nfeature, nindex)
        0             node.feature = nfeature
        0             node.index = nindex
      256             return nfeature+1, nindex+1
        0         elseif node.degree == 1
        0             nfeature, nindex = recurse(node.l, nfeature, nindex)
        0             node.feature = nfeature
        0             node.index = nindex
        0             return nfeature+1, nindex+1
        0         elseif node.degree == 0
        0             node.index = nindex
      384             return nfeature, nindex+1
        -         end
        -     end
        0     recurse(tree, 2, 1)
        - end
        - 
        - # a the nodes to be from 1 to the number of temporary nodes (meaning inputs and constants are not numbered)
        - # this removes the information of the original constant indices
        - @inline buffer_count_without_gradients(tree) = tree_mapreduce((n -> n.degree == 0 ? 1 : n.feature), max, tree)
        - @inline buffer_count_with_gradients(tree) = tree_mapreduce((n -> max(n.constant ? n.grad_ix : 1, n.degree == 0 ? 1 : n.feature)), max, tree)
        - @inline number_of_indices(tree) = tree_mapreduce((n -> n.index), max, tree)
        - 
        - function recalculate_constant!(tree::AbstractTensorExprNode)
        0     if tree.degree == 2
        0         recalculate_constant!(tree.l)
        0         recalculate_constant!(tree.r)
        0         tree.constant = tree.l.constant || tree.r.constant
        0     elseif tree.degree == 1
        0         recalculate_constant!(tree.l)
        0         tree.constant = tree.l.constant
        -     end
        - end
        - 
        - function recalculate_gradient_indices!(tree::AbstractTensorExprNode)
       32     b = buffer_count_without_gradients(tree)
        -     # maxf - maximum feature so far, 1 if no features
       32     function recurse(node, ix)
        0         if !node.constant
        0             node.grad_ix = 0
        0             return ix 
        -         end
        0         if node.degree == 1
        0             ix = recurse(node.l, ix)
        0         elseif node.degree == 2
        0             ix = recurse(node.l, ix)
        0             ix = recurse(node.r, ix)
        -         end
        0         node.grad_ix = ix
        0         return ix+1
        -     end
        0     recurse(tree, b+1)
        - end
        - 
        - function recalculate_node_values!(tree::AbstractTensorExprNode{T,N}, constants::FlattenedTensorList{T,N}) where {T,N}
        0     recalculate_constant_indices!(tree, constants)
        0     recalculate_constant!(tree)
        0     recalculate_node_indices!(tree)
        0     recalculate_gradient_indices!(tree)
        - end
        - 
        - function make_ftl_from_tree(tree::AbstractTensorExprNode{T,N}, buffer::AbstractVector{T}, maxB, ::Val{with_gradients}) where {T,N,with_gradients}
       32     bc = with_gradients ? buffer_count_with_gradients(tree) : buffer_count_without_gradients(tree)
      256     tempsizes = Vector{NTuple{N,Int32}}(undef, bc)
        0     tempsizes[1] = ntuple(Returns(1), Val(N))
       64     function recurse_set(node)
        0         if node.degree == 2
      128             recurse_set(node.l)
        0             recurse_set(node.r)
        0             tempsizes[node.feature] = node.shape
        0         elseif node.degree == 1
        0             recurse_set(node.l)
        0             tempsizes[node.feature] = node.shape
        -         end
        0         if with_gradients && node.constant
        0             tempsizes[node.grad_ix] = node.shape
        -         end
        -     end
       64     recurse_set(tree)
        0     return treat_as_flattened(buffer, tempsizes, maxB)
        - end
        - 
        - 
        - function reshape_constants(tree::AbstractTensorExprNode{T,N}, constants::FlattenedTensorList{T,N,IXT,AT}) where {T,N,IXT,AT}
        0     all_ok = tree_mapreduce(
        -         node -> is_node_constant(node) ? constants.positions[node.feature].shape == node.shape : true,
        -         &, tree, Bool
        -     )
        0     if all_ok return constants end
      160     v = Vector{NTuple{N,IXT}}(undef, count_constant_nodes(tree))
        0     tree_mapreduce(
        -         node -> begin
        -             if is_node_constant(node)
        -                 v[node.feature] = node.shape
        -             end
        -             return nothing
        -         end,
        -         Returns(nothing),
        -         tree, Nothing
        -     )
        0     B = constants.B
      288     buf = AT(undef, B*sum(prod, v))
        0     consts2 = treat_as_flattened(buf, v, B)
        0     for ci in eachindex(v)
        0         new_shape = consts2.positions[ci].shape
        0         new_len = consts2.positions[ci].len
        0         old_shape = constants.positions[ci].shape 
        0         old_len = constants.positions[ci].len
        0         if new_shape == old_shape
        0             mapk_ti!((_,b)->b, feature(consts2, ci), feature(constants, ci))
        0         elseif new_len == old_len
        0             mapk_ti!((_,b)->b, feature_flat(consts2, ci), feature_flat(constants, ci))
        0         elseif new_len < old_len
        0             mapk_ti!((_,b)->b, feature_flat(consts2, ci), selectdim_ti(feature_flat(constants, ci), 1:new_len))
        -         else
        -             off = 0
        0             while off <= new_len
        0                 if off+old_len > new_len
        0                     mapk_ti!((_,b)->b, 
        -                         selectdim_ti(feature_flat(consts2, ci), 1, (off+1):(new_len)), 
        -                         selectdim_ti(feature_flat(constants, ci), 1, 1:(new_len-off))
        -                     )
        -                 else
        0                     mapk_ti!((_,b)->b, 
        -                         selectdim_ti(feature_flat(consts2, ci), 1, (off+1):(off+old_len)), 
        -                         feature_flat(constants, ci)
        -                     )
        -                 end
        0                 off += old_len
        -             end
        -         end
        -         
        -         # ideally it would work like this:
        -         # first rule: if they have the same amount of elements, just keep them (equivalent to julia `reshape` function)
        -         # second rule: if they are broadcastable, broadcast them
        -         # if all(((n, o),) -> n == o || n == 1 || o == 1, zip(new_shape, old_shape))
        -         #     # should reduce?
        -         #     if any(((n, o),) -> n == 1 && o != 1, zip(new_shape, old_shape))
        -         #         consts2[ci] .= sum(constants[ci]; dims=
        -         #             filter(i -> new_shape[i] == 1 && old_shape[i] != 1, ntuple(i -> i, Val(N)))
        -         #         )
        -         #     else
        -         #         consts2[ci] .= constants[ci]
        -         #     end
        -         # end
        -         # third rule: get what dimensions are broadcastable, but clamp the rest
        -         
        0     end
        0     return consts2
        - end
        - 
        - end
